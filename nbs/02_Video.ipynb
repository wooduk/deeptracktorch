{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import cv2\n",
    "from fastai.vision import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def track_frame(\n",
    "    estimator,\n",
    "    frame,\n",
    "    box_half_size=25,\n",
    "    box_scanning_step=5,\n",
    "    ):\n",
    "    \"\"\"Tracks a frame box by box.\n",
    "    \n",
    "    Inputs:    \n",
    "    network: the pretrained network\n",
    "    frame: the frame to by analyzed\n",
    "    box_half_size: half the size of the scanning box\n",
    "    box_scanning_step: the size of the scanning step \n",
    "    \n",
    "    Output:\n",
    "    prediction_wrt_box: x, y and r coordiantes with respect to each box (pixels)\n",
    "    prediction_wrt_frame: x, y and r coordinates with respect to each frame (pixels)\n",
    "    boxes: the part of the frame corresponding to each box\n",
    "    \"\"\"  \n",
    "\n",
    "    import numpy as np\n",
    "    import cv2\n",
    "    \n",
    "    frame_width = frame.shape[1]\n",
    "    frame_height = frame.shape[0]\n",
    "    \n",
    "    \n",
    "    box_center_x = np.arange(box_half_size, \n",
    "                             frame_height - box_half_size, \n",
    "                             box_scanning_step)\n",
    "    box_center_y = np.arange(box_half_size, \n",
    "                             frame_width - box_half_size, \n",
    "                             box_scanning_step)\n",
    "\n",
    "\n",
    "    boxes = np.zeros((len(box_center_x), \n",
    "                      len(box_center_y),\n",
    "                      box_half_size * 2 + 1, \n",
    "                      box_half_size * 2 + 1))  \n",
    "    \n",
    "    print(boxes.shape)\n",
    "    prediction_wrt_box = np.zeros((len(box_center_x), \n",
    "                                   len(box_center_y), \n",
    "                                   3)) \n",
    "    prediction_wrt_frame = np.zeros((len(box_center_x), \n",
    "                                     len(box_center_y), \n",
    "                                     3)) \n",
    "    \n",
    "    \n",
    "    # Scanning over the frame row- and column-wise\n",
    "    for j in range(len(box_center_x)):\n",
    "        for k in range(len(box_center_y)):\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Define the scanning box\n",
    "            boxes[j, k] = frame[int(box_center_x[j] - box_half_size):int(box_center_x[j] + box_half_size + 1), \n",
    "                                int(box_center_y[k] - box_half_size):int(box_center_y[k] + box_half_size + 1)]\n",
    "            \n",
    "            box_predict = boxes[j, k]\n",
    "            \n",
    "            box_predict = cv2.resize(boxes[j, k], (51, 51))\n",
    "            \n",
    "            # Predict position of particle with respect to the scanning box\n",
    "            prediction_wrt_box[j, k] = estimator(box_predict)[1] #network.predict(np.reshape(box_predict, (1, 51, 51, 1)))\n",
    "\n",
    "            prediction_wrt_box[j, k][0] = prediction_wrt_box[j, k][0] * box_half_size + box_half_size\n",
    "            prediction_wrt_box[j, k][1] = prediction_wrt_box[j, k][1] * box_half_size + box_half_size\n",
    "            prediction_wrt_box[j, k][2] = prediction_wrt_box[j, k][2] * box_half_size\n",
    "\n",
    "            prediction_wrt_frame[j, k][0] = prediction_wrt_box[j, k][0] + box_scanning_step * j\n",
    "            prediction_wrt_frame[j, k][1] = prediction_wrt_box[j, k][1] + box_scanning_step * k\n",
    "            prediction_wrt_frame[j, k][2] = prediction_wrt_box[j, k][2]\n",
    "\n",
    "    return (prediction_wrt_box, prediction_wrt_frame, boxes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_GenerateSyntheticImages.ipynb.\n",
      "Converted 01_Models.ipynb.\n",
      "Converted 02_video.ipynb.\n",
      "Converted 03_measures.ipynb.\n",
      "Converted 98_Display.ipynb.\n",
      "Converted 99_cli.ipynb.\n",
      "Converted E1_Track1.ipynb.\n",
      "Converted E2_Track1fromN.ipynb.\n",
      "Converted E3_multipleparticles.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted tutorial_CNN.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def track(\n",
    "    video_file_name,\n",
    "    estimator,\n",
    "    number_frames_to_be_tracked=1,\n",
    "    box_half_size=25,\n",
    "    box_scanning_step=5,\n",
    "    frame_normalize=0,\n",
    "    frame_enhance=1):\n",
    "    \"\"\"Track multiple particles in a video.\n",
    "    \n",
    "    Inputs:    \n",
    "    video_file_name: video file\n",
    "    network: the pre-trained network\n",
    "    number_frames_to_be_tracked: number of frames to by analyzed from video begining. If number_frames is equal to 0 then the whole video is tracked.\n",
    "    box_half_size: half the size of the scanning box. If box_half_size is equal to 0 then a single particle is tracked in a frame.\n",
    "    box_scanning_step: the size of the scanning step\n",
    "    frame_normalize: option to normalize the frame before tracking.\n",
    "    frame_enhance: option to enhance the frame before tracking.\n",
    "    \n",
    "    Output:\n",
    "    frames: frames from video\n",
    "    predicted_positions_wrt_frame: x, y and r coordinates with respect to the all frames (pixels) \n",
    "    predicted_positions_wrt_box: x, y and r coordinates with respect to the boxes for all frames (pixels) \n",
    "    boxes_all: the part of the frame corresponding to each box for all frames\n",
    "    number_frames_to_be_tracked: the number of frames that have been tracked. \n",
    "    \"\"\"\n",
    "    \n",
    "    import cv2\n",
    "    import numpy as np\n",
    "\n",
    "    # Read the video file and its properties\n",
    "    video = cv2.VideoCapture(video_file_name)\n",
    "\n",
    "    if number_frames_to_be_tracked == 0:\n",
    "        number_frames_to_be_tracked = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    video_height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    video_width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "\n",
    "    # Initialize variables\n",
    "    frames = np.zeros((number_frames_to_be_tracked, video_height, video_width))\n",
    "\n",
    "   \n",
    "    box_center_x = np.arange(box_half_size, \n",
    "                             video_height - box_half_size, \n",
    "                             box_scanning_step)\n",
    "    box_center_y = np.arange(box_half_size, \n",
    "                             video_width - box_half_size, \n",
    "                             box_scanning_step)\n",
    "\n",
    "\n",
    "    boxes_all = np.zeros((number_frames_to_be_tracked, \n",
    "                          len(box_center_x),\n",
    "                          len(box_center_y), \n",
    "                          box_half_size * 2 + 1,\n",
    "                          box_half_size * 2 + 1))\n",
    "\n",
    "    predicted_positions_wrt_box = np.zeros((number_frames_to_be_tracked,\n",
    "                                           len(box_center_x), \n",
    "                                           len(box_center_y), \n",
    "                                           3))\n",
    "    predicted_positions_wrt_frame = np.zeros((number_frames_to_be_tracked,\n",
    "                                             len(box_center_x), \n",
    "                                             len(box_center_y), \n",
    "                                             3))\n",
    "\n",
    "    # Track the positions of the particles frame by frame\n",
    "    for i in range(number_frames_to_be_tracked):\n",
    "\n",
    "        # Read the current frame from the video\n",
    "        (ret, frame) = video.read()\n",
    "        \n",
    "        # Normalize the frame\n",
    "        if frame_normalize == 1:\n",
    "            frame = cv2.normalize(frame, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "        # Convert color image to grayscale.\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) / 255\n",
    "        \n",
    "        frame = frame * frame_enhance\n",
    "\n",
    "        # Generate the scanning boxes and predict particle position in each box\n",
    "\n",
    "        (prediction_wrt_box, prediction_wrt_frame, boxes) = track_frame(estimator, frame, box_half_size, box_scanning_step)\n",
    "\n",
    "\n",
    "        frames[i] = frame\n",
    "            \n",
    "        predicted_positions_wrt_box[i] = prediction_wrt_box\n",
    "        predicted_positions_wrt_frame[i] = prediction_wrt_frame\n",
    "        boxes_all[i] = boxes\n",
    "\n",
    "    # Release the video\n",
    "    video.release()\n",
    "    \n",
    "\n",
    "    return (number_frames_to_be_tracked, frames, predicted_positions_wrt_frame, predicted_positions_wrt_box, boxes_all)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def plot_tracked_scanning_boxes(\n",
    "    frame_to_be_shown,\n",
    "    rows_to_be_shown,\n",
    "    columns_to_be_shown,\n",
    "    boxes_all,\n",
    "    predicted_positions_wrt_box,\n",
    "    box_half_size=25,\n",
    "    ): \n",
    "    \"\"\"Plot tracked scanning boxes over a range of frames.\n",
    "    \n",
    "    Inputs:    \n",
    "    frame_to_be_shown: the range of frames to be shown\n",
    "    rows_to_be_shown: the range of rows to be shown \n",
    "    columns_to_be_shown: the range of columns to be shown \n",
    "    boxes_all: the part of the frame corresponding to each box for all frames\n",
    "    predicted_positions_wrt_box: x, y and r coordinates with respect to boxes for all frames (pixels)\n",
    "    box_half_size: half the size of the scanning box\n",
    "    \n",
    "    Output: none\n",
    "    \"\"\"\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "       \n",
    "    plt.figure(10)\n",
    "\n",
    "    for i in list(frame_to_be_shown):\n",
    "        for j in list(rows_to_be_shown):\n",
    "            for k in list(columns_to_be_shown):\n",
    "\n",
    "                plt.subplot(1, 2, 1)\n",
    "                plt.imshow(boxes_all[i, j, k],\n",
    "                           cmap='gray', \n",
    "                           vmin=0, \n",
    "                           vmax=1)\n",
    "                \n",
    "                plt.plot(predicted_positions_wrt_box[i, j, k, 1],\n",
    "                         predicted_positions_wrt_box[i, j, k, 0], \n",
    "                         'ob')\n",
    "                plt.xlabel('y (px)', fontsize=16)\n",
    "                plt.ylabel('x (px)', fontsize=16)\n",
    "                \n",
    "                \n",
    "                plt.subplot(1, 2, 2)\n",
    "\n",
    "                plt.text(0, .8, 'frame = %1.0f' % i, fontsize=16)\n",
    "                plt.text(0, .7, 'row = %1.0f' % j, fontsize=16)\n",
    "                plt.text(0, .6, 'column = %1.0f' % k, fontsize=16)\n",
    "                \n",
    "                plt.text(0, .4, 'particle center x = %5.2f px' % predicted_positions_wrt_box[i, j, k, 0], \n",
    "                         fontsize=16, color='b')\n",
    "                plt.text(0, .3, 'particle center y = %5.2f px' % predicted_positions_wrt_box[i, j, k, 1], \n",
    "                         fontsize=16, color='b')\n",
    "                plt.text(0, .2, 'particle radius = %5.2f px' % predicted_positions_wrt_box[i, j, k, 2], \n",
    "                         fontsize=16, color='b')\n",
    "                plt.axis('off')\n",
    "\n",
    "                \n",
    "                plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ones()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def centroids(\n",
    "    particle_positions_x,\n",
    "    particle_positions_y,\n",
    "    particle_radial_distance,\n",
    "    particle_interdistance,\n",
    "    ):\n",
    "    \"\"\"Calculate centroid of the particles by taking the mean x and y positions.\n",
    "    \n",
    "    Inputs:    \n",
    "    x_particle_positions: the predicted x-positions for the particles (many for each particle)\n",
    "    y_particle_positions: the predicted y-positions for the particles (many for each particle)\n",
    "    particle_radial_distance: the radial distance of the particle from the center of the scanning box\n",
    "    particle_max_interdistance: the maximum distance between predicted points for them to belong to the same particle\n",
    "    \n",
    "    Output: \n",
    "    x_centroid: the x coordinate of the centroid for each particle \n",
    "    y_centroid: the y coordinate of the centroid for each particle\n",
    "    \"\"\"\n",
    "\n",
    "    import numpy as np\n",
    "\n",
    "    def ppos_same_particle(p1,p2):\n",
    "        return ((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2) < rmin\n",
    "            \n",
    "    particle_number = 0\n",
    "    particle_index = 0\n",
    "    particle_numbers = -np.ones(len(particle_positions_x)) #[-1,-1,-1,-1]\n",
    "    \n",
    "    # Sort all predicted points to correct particle \n",
    "    \n",
    "    # number the particles\n",
    "    while particle_numbers[np.argmin(particle_numbers)] == -1:\n",
    "        particle_index = np.argmin(particle_numbers) # index of lowest particle num\n",
    "        particle_numbers[particle_index] = particle_number # value of lowest particle num\n",
    "        particle_number += 1 # effectively 'select' this particle\n",
    "\n",
    "        for j in range(len(particle_positions_x)):\n",
    "\n",
    "            if ppos_same_particle(ppos[j], ppos[pi], particle_rmin):\n",
    "                particle_numbers[j] = particle_numbers[particle_index]\n",
    "                \n",
    "#             if (particle_positions_x[j] - particle_positions_x[particle_index]) ** 2 \\\n",
    "#                 + (particle_positions_y[j] - particle_positions_y[particle_index]) ** 2 \\\n",
    "#                 < particle_interdistance ** 2:\n",
    "#                 particle_numbers[j] = particle_numbers[particle_index]\n",
    "\n",
    "    centroid_x = np.zeros(int(np.amax(particle_numbers)) + 1)\n",
    "    centroid_y = np.zeros(int(np.amax(particle_numbers)) + 1)\n",
    "\n",
    "    particle_number = 0\n",
    "    while max(particle_numbers) >= particle_number:\n",
    "        points_x = particle_positions_x[np.where(particle_numbers == particle_number)]\n",
    "        points_y = particle_positions_y[np.where(particle_numbers == particle_number)]\n",
    "        distance_from_center = particle_radial_distance[np.where(particle_numbers == particle_number)]\n",
    "\n",
    "        # Calculate centroids\n",
    "        _len = len(points_x)\n",
    "        centroid_x[particle_number] = sum(points_x) / _len\n",
    "        centroid_y[particle_number] = sum(points_y) / _len\n",
    "\n",
    "        particle_number += 1\n",
    "\n",
    "    return (centroid_x, centroid_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def show_tracked_frames(\n",
    "    particle_radial_distance_threshold,\n",
    "    particle_maximum_interdistance,\n",
    "    number_frames_to_be_shown,\n",
    "    frames,\n",
    "    predicted_positions_wrt_frame,\n",
    "    ):\n",
    "    \"\"\"Show the frames with the predicted positions and centroid positions.\n",
    "    \n",
    "    Inputs:    \n",
    "    particle_radial_distance: the radial distance of the particle from the center of the scanning box\n",
    "    particle_maximuminterdistance: the maximum distance between predicted points for them to belong to the same particle\n",
    "    number_frames_to_be_shown: number of frames to be shown\n",
    "    frames: frames from video\n",
    "    predicted_positions_wrt_frame: x, y and r coordinates with respect to frames (pixels) \n",
    "    \n",
    "    Output: none\n",
    "    \"\"\"\n",
    "    \n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    particle_positions = []\n",
    "    particle_centroids = []\n",
    "    particle_radial_distance = []\n",
    "\n",
    "    for i in range(number_frames_to_be_shown):\n",
    "\n",
    "        particle_positions_x = []\n",
    "        particle_positions_y = []\n",
    "\n",
    "        # Show frame\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(frames[i], cmap='gray', vmin=0, vmax=1)\n",
    "\n",
    "        # Threshold the radial distance of the predicted points\n",
    "        for j in range(0, predicted_positions_wrt_frame.shape[1]):\n",
    "            for k in range(0, predicted_positions_wrt_frame.shape[2]):\n",
    "\n",
    "                if predicted_positions_wrt_frame[i, j, k, 2] \\\n",
    "                    < particle_radial_distance_threshold:\n",
    "\n",
    "                    # Plot the predicted points\n",
    "                    plt.plot(predicted_positions_wrt_frame[i, j, k, 1],\n",
    "                             predicted_positions_wrt_frame[i, j, k, 0], '.b')\n",
    "                    particle_positions_x = \\\n",
    "                        np.append(particle_positions_x,\n",
    "                                  predicted_positions_wrt_frame[i, j, k, 0])\n",
    "                    particle_positions_y = \\\n",
    "                        np.append(particle_positions_y,\n",
    "                                  predicted_positions_wrt_frame[i, j, k, 1])\n",
    "                    particle_radial_distance = \\\n",
    "                        np.append(particle_radial_distance,\n",
    "                                  predicted_positions_wrt_frame[i, j, k, 2])\n",
    "\n",
    "        particle_positions.append([])\n",
    "        particle_positions[i].append(particle_positions_x)\n",
    "        particle_positions[i].append(particle_positions_y)\n",
    "\n",
    "        # Calculate the centroid positions\n",
    "        (centroids_x, centroids_y) = centroids(particle_positions_x,\n",
    "                                               particle_positions_y, \n",
    "                                               particle_radial_distance,\n",
    "                                               particle_maximum_interdistance)\n",
    "\n",
    "        particle_centroids.append([])\n",
    "        particle_centroids[i].append(centroids_x)\n",
    "        particle_centroids[i].append(centroids_y)\n",
    "\n",
    "        # Plot the centroid positions\n",
    "        plt.plot(\n",
    "            centroids_y,\n",
    "            centroids_x,\n",
    "            'o',\n",
    "            color='#e6661a',\n",
    "            fillstyle='none',\n",
    "            markersize=10,\n",
    "            )\n",
    "        \n",
    "    return #(particle_positions, particle_centroids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def track_single_particle(\n",
    "    video_file_name,\n",
    "    estimators,\n",
    "    number_frames_to_be_tracked=0,\n",
    "    frame_normalize=0,\n",
    "    frame_enhance=1,\n",
    "    use_cv2=False):\n",
    "    \"\"\"Track single particlee in a video.\n",
    "    \n",
    "    Inputs:    \n",
    "    video_file_name: video file\n",
    "    network: the pre-trained network\n",
    "    number_frames_to_be_tracked: number of frames to by analyzed from video begining. If number_frames is equal to 0 then the whole video is tracked.\n",
    "        \n",
    "    Output:\n",
    "    frames: frames from video\n",
    "    predicted_positions: x, y and r coordinates with respect to the all frames (pixels) \n",
    "    \"\"\"\n",
    "    \n",
    "    from deeptracktorch.simg import npimg2tensor\n",
    "\n",
    "\n",
    "\n",
    "    if use_cv2:\n",
    "        \n",
    "        # Read the video file and its properties\n",
    "        video = cv2.VideoCapture(video_file_name)\n",
    "        fps = video.get(cv2.CAP_PROP_FPS)\n",
    "        print(fps)\n",
    "\n",
    "        if number_frames_to_be_tracked == 0:\n",
    "            number_frames_to_be_tracked = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        video_height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        video_width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))  \n",
    "\n",
    "        # Track the positions of the particles frame by frame\n",
    "        for i in range(number_frames_to_be_tracked):\n",
    "\n",
    "            # Read the current frame from the video\n",
    "            (ret, frame) = video.read()\n",
    "\n",
    "            # Normalize the frame\n",
    "            if frame_normalize == 1:\n",
    "                frame = cv2.normalize(frame, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "            # Convert color image to grayscale.\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) / 255\n",
    "\n",
    "            # Enhance the frame\n",
    "            frame = frame * frame_enhance\n",
    "\n",
    "            frame = cv2.resize(frame,(51,51))\n",
    "\n",
    "            scale_x = video_width / 51\n",
    "            scale_y = video_height / 51\n",
    "\n",
    "            estimated_positions = {k:np.empty((number_frames_to_be_tracked,3)) for (k,v) in estimators.items()}\n",
    "\n",
    "            # apply methods to estimate particle positions\n",
    "            image = np.array(frame)\n",
    "\n",
    "            for e in estimators: estimated_positions[e][i,:]=estimators[e](frame)\n",
    "    \n",
    "    else:\n",
    "        import av\n",
    "        container = av.open(video_file_name)\n",
    "\n",
    "        if number_frames_to_be_tracked == 0:\n",
    "            number_frames_to_be_tracked = container.streams.video[0].frames\n",
    "        \n",
    "        estimated_positions = {k:np.empty((number_frames_to_be_tracked,3)) for (k,v) in estimators.items()}\n",
    "\n",
    "        for i, frame in enumerate(container.decode(video=0)):\n",
    "            image = frame.to_image().resize((51,51)).convert('L')\n",
    "            image = np.array(image) / 255\n",
    "    \n",
    "            for e in estimators: estimated_positions[e][i,:]=estimators[e](image)\n",
    "    \n",
    "    return estimated_positions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'learner' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-e5574ba32690>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m (number_tracked_frames, frames, predicted_positions) = track_single_particle(\n\u001b[1;32m     13\u001b[0m     \u001b[0mvideo_file_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mlearner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mnumber_frames_to_be_tracked\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mframe_normalize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'learner' is not defined"
     ]
    }
   ],
   "source": [
    "### Define the video file to be tracked\n",
    "video_file_name = '../../DeepTrack 1.0/DeepTrack - Example 2 - Optically Trapped Particle Good.mp4'\n",
    "\n",
    "### Define the number of frames to be tracked\n",
    "number_frames_to_be_tracked = 2\n",
    "\n",
    "### Preprocess the images\n",
    "frame_normalize = 0\n",
    "frame_enhance = 1\n",
    "\n",
    "### Track the video\n",
    "(number_tracked_frames, frames, predicted_positions) = track_single_particle(\n",
    "    video_file_name, \n",
    "    learner, \n",
    "    number_frames_to_be_tracked,\n",
    "    frame_normalize,\n",
    "    frame_enhance)\n",
    "\n",
    "print(predicted_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def show_tracked_frames_single_particle(\n",
    "    number_frames_to_be_shown,\n",
    "    frames,\n",
    "    predicted_positions,\n",
    "    ):\n",
    "    \"\"\"Show the frames with the predicted position.\n",
    "    \n",
    "    Inputs:    \n",
    "    number_frames_to_be_shown: number of frames to be shown\n",
    "    frames: frames from video\n",
    "    predicted_positions: x, y and r coordinates (pixels) \n",
    "    \n",
    "    Output: none\n",
    "    \"\"\"\n",
    "    \n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "    for i in range(number_frames_to_be_shown):\n",
    "\n",
    "        # Show frame\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(frames[i], cmap='gray', vmin=0, vmax=1)\n",
    "\n",
    "\n",
    "        # Plot the predicted points\n",
    "        plt.plot(predicted_positions[i, 1],\n",
    "                 predicted_positions[i, 0], \n",
    "                 '.',\n",
    "                 color='#e6661a',\n",
    "                 #fillstyle='none',\n",
    "                 markersize=20,\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def particle_positions(\n",
    "    particle_number=2,\n",
    "    first_particle_range=0.5,\n",
    "    other_particle_range=1,\n",
    "    particle_distance=50,\n",
    "    ):\n",
    "    \"\"\"Generates multiple particle x- and y-coordinates with respect to each other.\n",
    "    \n",
    "    Inputs:  \n",
    "    particle_number: number of particles to generate coordinates for\n",
    "    first_particle_range: allowed x- and y-range of the centermost particle\n",
    "    other_particle_range: allowed x- and y-range for all other particles\n",
    "    particle_distance: particle interdistance\n",
    "    \n",
    "    Output:\n",
    "    particles_center_x: list of x-coordinates for the particles\n",
    "    particles_center_y: list of y-coordinates for the particles\n",
    "    \"\"\"\n",
    "\n",
    "    from numpy.random import normal\n",
    "    from numpy.random import uniform\n",
    "    from numpy import insert\n",
    "    from numpy import array\n",
    "    from itertools import combinations\n",
    "    from numpy import linalg\n",
    "    \n",
    "    ### Centermost particle\n",
    "    target_particle_center_x = uniform(-first_particle_range,\n",
    "            first_particle_range)\n",
    "    target_particle_center_y = uniform(-first_particle_range,\n",
    "            first_particle_range)\n",
    "\n",
    "    target_center_distance = (target_particle_center_x ** 2\n",
    "                              + target_particle_center_y ** 2) ** 0.5\n",
    "\n",
    "    ### Other particles\n",
    "    while True:\n",
    "        particles_center_x = uniform(-other_particle_range,\n",
    "                other_particle_range, particle_number - 1)  \n",
    "        particles_center_y = uniform(-other_particle_range,\n",
    "                other_particle_range, particle_number - 1) \n",
    "\n",
    "        center_distance = (particles_center_x ** 2 + particles_center_y\n",
    "                           ** 2) ** 0.5\n",
    "\n",
    "        ### Force all other particles to be further away from the center than the centermost particle\n",
    "        if any(t < target_center_distance for t in center_distance):\n",
    "            continue\n",
    "\n",
    "        particles_center_x = insert(particles_center_x, 0,\n",
    "                                    target_particle_center_x)\n",
    "        particles_center_y = insert(particles_center_y, 0,\n",
    "                                    target_particle_center_y)\n",
    "\n",
    "        particle_centers = array([particles_center_x,\n",
    "                                 particles_center_y])\n",
    "\n",
    "        ### Force all particles to be a certain distance from each other\n",
    "        if all(linalg.norm(p - q) > particle_distance for (p, q) in\n",
    "               combinations(particle_centers, 2)):\n",
    "            break\n",
    "\n",
    "    return (particles_center_x, particles_center_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
