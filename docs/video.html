---

title: Title

keywords: fastai
sidebar: home_sidebar



---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/02_Video.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="track_frame" class="doc_header"><code>track_frame</code><a href="https://github.com/wooduk/deeptracktorch/tree/master/deeptracktorch/video.py#L13" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>track_frame</code>(<strong><code>estimator</code></strong>, <strong><code>frame</code></strong>, <strong><code>box_half_size</code></strong>=<em><code>25</code></em>, <strong><code>box_scanning_step</code></strong>=<em><code>5</code></em>)</p>
</blockquote>
<p>Tracks a frame box by box.</p>
<p>Inputs:
network: the pretrained network
frame: the frame to by analyzed
box_half_size: half the size of the scanning box
box_scanning_step: the size of the scanning step</p>
<p>Output:
prediction_wrt_box: x, y and r coordiantes with respect to each box (pixels)
prediction_wrt_frame: x, y and r coordinates with respect to each frame (pixels)
boxes: the part of the frame corresponding to each box</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="track" class="doc_header"><code>track</code><a href="https://github.com/wooduk/deeptracktorch/tree/master/deeptracktorch/video.py#L91" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>track</code>(<strong><code>video_file_name</code></strong>, <strong><code>estimator</code></strong>, <strong><code>number_frames_to_be_tracked</code></strong>=<em><code>1</code></em>, <strong><code>box_half_size</code></strong>=<em><code>25</code></em>, <strong><code>box_scanning_step</code></strong>=<em><code>5</code></em>, <strong><code>frame_normalize</code></strong>=<em><code>0</code></em>, <strong><code>frame_enhance</code></strong>=<em><code>1</code></em>)</p>
</blockquote>
<p>Track multiple particles in a video.</p>
<p>Inputs:
video_file_name: video file
network: the pre-trained network
number_frames_to_be_tracked: number of frames to by analyzed from video begining. If number_frames is equal to 0 then the whole video is tracked.
box_half_size: half the size of the scanning box. If box_half_size is equal to 0 then a single particle is tracked in a frame.
box_scanning_step: the size of the scanning step
frame_normalize: option to normalize the frame before tracking.
frame_enhance: option to enhance the frame before tracking.</p>
<p>Output:
frames: frames from video
predicted_positions_wrt_frame: x, y and r coordinates with respect to the all frames (pixels)
predicted_positions_wrt_box: x, y and r coordinates with respect to the boxes for all frames (pixels)
boxes_all: the part of the frame corresponding to each box for all frames
number_frames_to_be_tracked: the number of frames that have been tracked.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="plot_tracked_scanning_boxes" class="doc_header"><code>plot_tracked_scanning_boxes</code><a href="https://github.com/wooduk/deeptracktorch/tree/master/deeptracktorch/video.py#L192" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>plot_tracked_scanning_boxes</code>(<strong><code>frame_to_be_shown</code></strong>, <strong><code>rows_to_be_shown</code></strong>, <strong><code>columns_to_be_shown</code></strong>, <strong><code>boxes_all</code></strong>, <strong><code>predicted_positions_wrt_box</code></strong>, <strong><code>box_half_size</code></strong>=<em><code>25</code></em>)</p>
</blockquote>
<p>Plot tracked scanning boxes over a range of frames.</p>
<p>Inputs:
frame_to_be_shown: the range of frames to be shown
rows_to_be_shown: the range of rows to be shown
columns_to_be_shown: the range of columns to be shown
boxes_all: the part of the frame corresponding to each box for all frames
predicted_positions_wrt_box: x, y and r coordinates with respect to boxes for all frames (pixels)
box_half_size: half the size of the scanning box</p>
<p>Output: none</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="centroids" class="doc_header"><code>centroids</code><a href="https://github.com/wooduk/deeptracktorch/tree/master/deeptracktorch/video.py#L255" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>centroids</code>(<strong><code>particle_positions_x</code></strong>, <strong><code>particle_positions_y</code></strong>, <strong><code>particle_radial_distance</code></strong>, <strong><code>particle_interdistance</code></strong>)</p>
</blockquote>
<p>Calculate centroid of the particles by taking the mean x and y positions.</p>
<p>Inputs:
x_particle_positions: the predicted x-positions for the particles (many for each particle)
y_particle_positions: the predicted y-positions for the particles (many for each particle)
particle_radial_distance: the radial distance of the particle from the center of the scanning box
particle_max_interdistance: the maximum distance between predicted points for them to belong to the same particle</p>
<p>Output:
x_centroid: the x coordinate of the centroid for each particle
y_centroid: the y coordinate of the centroid for each particle</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="show_tracked_frames" class="doc_header"><code>show_tracked_frames</code><a href="https://github.com/wooduk/deeptracktorch/tree/master/deeptracktorch/video.py#L321" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>show_tracked_frames</code>(<strong><code>particle_radial_distance_threshold</code></strong>, <strong><code>particle_maximum_interdistance</code></strong>, <strong><code>number_frames_to_be_shown</code></strong>, <strong><code>frames</code></strong>, <strong><code>predicted_positions_wrt_frame</code></strong>)</p>
</blockquote>
<p>Show the frames with the predicted positions and centroid positions.</p>
<p>Inputs:
particle_radial_distance: the radial distance of the particle from the center of the scanning box
particle_maximuminterdistance: the maximum distance between predicted points for them to belong to the same particle
number_frames_to_be_shown: number of frames to be shown
frames: frames from video
predicted_positions_wrt_frame: x, y and r coordinates with respect to frames (pixels)</p>
<p>Output: none</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="track_single_particle" class="doc_header"><code>track_single_particle</code><a href="https://github.com/wooduk/deeptracktorch/tree/master/deeptracktorch/video.py#L404" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>track_single_particle</code>(<strong><code>video_file_name</code></strong>, <strong><code>estimators</code></strong>, <strong><code>number_frames_to_be_tracked</code></strong>=<em><code>0</code></em>, <strong><code>frame_normalize</code></strong>=<em><code>0</code></em>, <strong><code>frame_enhance</code></strong>=<em><code>1</code></em>, <strong><code>use_cv2</code></strong>=<em><code>False</code></em>)</p>
</blockquote>
<p>Track single particlee in a video.</p>
<p>Inputs:
video_file_name: video file
network: the pre-trained network
number_frames_to_be_tracked: number of frames to by analyzed from video begining. If number_frames is equal to 0 then the whole video is tracked.</p>
<p>Output:
frames: frames from video
predicted_positions: x, y and r coordinates with respect to the all frames (pixels)</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#notest</span>
<span class="c1">### Define the video file to be tracked</span>
<span class="n">video_file_name</span> <span class="o">=</span> <span class="s1">&#39;../../DeepTrack 1.0/DeepTrack - Example 2 - Optically Trapped Particle Good.mp4&#39;</span>

<span class="c1">### Define the number of frames to be tracked</span>
<span class="n">number_frames_to_be_tracked</span> <span class="o">=</span> <span class="mi">2</span>

<span class="c1">### Preprocess the images</span>
<span class="n">frame_normalize</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">frame_enhance</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1">### Track the video</span>
<span class="p">(</span><span class="n">number_tracked_frames</span><span class="p">,</span> <span class="n">frames</span><span class="p">,</span> <span class="n">predicted_positions</span><span class="p">)</span> <span class="o">=</span> <span class="n">track_single_particle</span><span class="p">(</span>
    <span class="n">video_file_name</span><span class="p">,</span> 
    <span class="n">learner</span><span class="p">,</span> 
    <span class="n">number_frames_to_be_tracked</span><span class="p">,</span>
    <span class="n">frame_normalize</span><span class="p">,</span>
    <span class="n">frame_enhance</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">predicted_positions</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-37-e5574ba32690&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span>
<span class="ansi-green-intense-fg ansi-bold">     12</span> (number_tracked_frames, frames, predicted_positions) = track_single_particle(
<span class="ansi-green-intense-fg ansi-bold">     13</span>     video_file_name<span class="ansi-blue-fg">,</span>
<span class="ansi-green-fg">---&gt; 14</span><span class="ansi-red-fg">     </span>learner<span class="ansi-blue-fg">,</span>
<span class="ansi-green-intense-fg ansi-bold">     15</span>     number_frames_to_be_tracked<span class="ansi-blue-fg">,</span>
<span class="ansi-green-intense-fg ansi-bold">     16</span>     frame_normalize<span class="ansi-blue-fg">,</span>

<span class="ansi-red-fg">NameError</span>: name &#39;learner&#39; is not defined</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="show_tracked_frames_single_particle" class="doc_header"><code>show_tracked_frames_single_particle</code><a href="https://github.com/wooduk/deeptracktorch/tree/master/deeptracktorch/video.py#L488" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>show_tracked_frames_single_particle</code>(<strong><code>number_frames_to_be_shown</code></strong>, <strong><code>frames</code></strong>, <strong><code>predicted_positions</code></strong>)</p>
</blockquote>
<p>Show the frames with the predicted position.</p>
<p>Inputs:
number_frames_to_be_shown: number of frames to be shown
frames: frames from video
predicted_positions: x, y and r coordinates (pixels)</p>
<p>Output: none</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="particle_positions" class="doc_header"><code>particle_positions</code><a href="https://github.com/wooduk/deeptracktorch/tree/master/deeptracktorch/video.py#L524" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>particle_positions</code>(<strong><code>particle_number</code></strong>=<em><code>2</code></em>, <strong><code>first_particle_range</code></strong>=<em><code>0.5</code></em>, <strong><code>other_particle_range</code></strong>=<em><code>1</code></em>, <strong><code>particle_distance</code></strong>=<em><code>50</code></em>)</p>
</blockquote>
<p>Generates multiple particle x- and y-coordinates with respect to each other.</p>
<p>Inputs:
particle_number: number of particles to generate coordinates for
first_particle_range: allowed x- and y-range of the centermost particle
other_particle_range: allowed x- and y-range for all other particles
particle_distance: particle interdistance</p>
<p>Output:
particles_center_x: list of x-coordinates for the particles
particles_center_y: list of y-coordinates for the particles</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

